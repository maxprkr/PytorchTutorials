{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)\n",
        "\n",
        "\n",
        "**Author:** [Driss Guessous](https://github.com/drisspg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we want to highlight a new ``torch.nn.functional`` function\n",
        "that can be helpful for implementing transformer architectures. The\n",
        "function is named ``torch.nn.functional.scaled_dot_product_attention``.\n",
        "For detailed description of the function, see the [PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention)_.\n",
        "This function has already been incorporated into ``torch.nn.MultiheadAttention`` and ``torch.nn.TransformerEncoderLayer``.\n",
        "\n",
        "## Overview\n",
        "At a high level, this PyTorch function calculates the\n",
        "scaled dot product attention (SDPA) between query, key, and value according to\n",
        "the definition found in the paper [Attention is all you\n",
        "need](https://arxiv.org/abs/1706.03762)_. While this function can\n",
        "be written in PyTorch using existing functions, a fused implementation can provide\n",
        "large performance benefits over a naive implementation.\n",
        "\n",
        "## Fused implementations\n",
        "\n",
        "For CUDA tensor inputs, the function will dispatch into one of the following\n",
        "implementations:\n",
        "\n",
        "* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)_\n",
        "* [Memory-Efficient Attention](https://github.com/facebookresearch/xformers)_\n",
        "* A PyTorch implementation defined in C++\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial requires PyTorch 2.0.0 or later.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.2811, -0.0906,  0.8079, -0.3959, -0.0777,  0.7030, -0.7100,\n",
              "           0.2185],\n",
              "         [-0.1866,  0.2599,  1.1457,  0.5540, -0.2537,  0.4580, -0.7290,\n",
              "          -0.2719],\n",
              "         [-0.4372,  0.1524,  1.1037,  0.6069, -0.0507,  0.4431, -0.5212,\n",
              "          -0.2084]],\n",
              "\n",
              "        [[-0.6468,  1.1123,  0.5440,  0.1510,  0.7588,  1.1830,  1.0656,\n",
              "           1.2469],\n",
              "         [-0.7013,  1.2121,  0.3516,  0.1248,  0.6938,  0.8001,  0.8924,\n",
              "           0.9111],\n",
              "         [-1.1531,  1.0549, -0.1383, -0.4782,  0.6345, -0.5737,  0.0310,\n",
              "          -0.0857]]], device='cuda:0')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Example Usage:\n",
        "query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
        "F.scaled_dot_product_attention(query, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicit Dispatcher Control\n",
        "\n",
        "While the function will implicitly dispatch to one of the three\n",
        "implementations, the user can also explicitly control the dispatch via\n",
        "the use of a context manager. This context manager allows users to\n",
        "explicitly disable certain implementations. If a user wants to ensure\n",
        "the function is indeed using the fastest implementation for their\n",
        "specific inputs, the context manager can be used to sweep through\n",
        "measuring performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Lets define a helpful benchmarking function:\n",
        "import torch.utils.benchmark as benchmark\n",
        "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
        "    t0 = benchmark.Timer(\n",
        "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
        "    )\n",
        "    return t0.blocked_autorange().mean * 1e6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Lets define the hyper-parameters of our input\n",
        "batch_size = 32\n",
        "max_sequence_len = 1024\n",
        "num_heads = 32\n",
        "embed_dimension = 32\n",
        "\n",
        "dtype = torch.float64\n",
        "\n",
        "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
        "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
        "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 9.04 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbenchmark_torch_function_in_microseconds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mbenchmark_torch_function_in_microseconds\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark_torch_function_in_microseconds\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      4\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[0;32m      5\u001b[0m         stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf(*args, **kwargs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m: f}\n\u001b[0;32m      6\u001b[0m     )\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocked_autorange\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39\\lib\\site-packages\\torch\\utils\\benchmark\\utils\\timer.py:394\u001b[0m, in \u001b[0;36mTimer.blocked_autorange\u001b[1;34m(self, callback, min_run_time)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblocked_autorange\u001b[39m(\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    355\u001b[0m     callback: Optional[Callable[[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], NoReturn]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    356\u001b[0m     min_run_time: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m    357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m common\u001b[38;5;241m.\u001b[39mMeasurement:\n\u001b[0;32m    358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Measure many replicates while keeping timer overhead to a minimum.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    At a high level, blocked_autorange executes the following pseudo-code::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m        (mean, median, etc.)\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_block_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_run_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_hook\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeit(number)\n",
            "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39\\lib\\site-packages\\torch\\utils\\benchmark\\utils\\timer.py:311\u001b[0m, in \u001b[0;36mTimer._estimate_block_size\u001b[1;34m(self, min_run_time)\u001b[0m\n\u001b[0;32m    309\u001b[0m number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     time_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m     relative_overhead \u001b[38;5;241m=\u001b[39m overhead \u001b[38;5;241m/\u001b[39m time_taken\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m relative_overhead \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m time_taken \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_run_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39\\lib\\site-packages\\torch\\utils\\benchmark\\utils\\timer.py:256\u001b[0m, in \u001b[0;36mTimer._timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_timeit\u001b[39m(\u001b[38;5;28mself\u001b[39m, number: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# Even calling a timer in C++ takes ~50 ns, so no real operation should\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# take less than 1 ns. (And this prevents divide by zero errors.)\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1e-9\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39\\lib\\timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    175\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
            "File \u001b[1;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 9.04 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hardware dependence\n",
        "\n",
        "Depending on what machine you ran the above cell on and what hardware is\n",
        "available, your results might be different.\n",
        "- If you donâ€™t have a GPU and are running on CPU then the context manager\n",
        "will have no effect and all three runs should return similar timings.\n",
        "- Depending on what compute capability your graphics card supports\n",
        "flash attention or memory efficient might have failed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Causal Self Attention\n",
        "\n",
        "Below is an example implementation of a multi-headed causal self\n",
        "attention block inspired by\n",
        "[Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ repository.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CausalSelfAttention(\n",
            "  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
            "  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n",
        "        super().__init__()\n",
        "        assert embed_dimension % num_heads == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n",
        "        # regularization\n",
        "        self.dropout = dropout\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dimension = embed_dimension\n",
        "        # Perform causal masking\n",
        "        self.is_causal = is_causal\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        query_projected = self.c_attn(x)\n",
        "\n",
        "        batch_size = query_projected.size(0)\n",
        "        embed_dim = query_projected.size(2)\n",
        "        head_dim = embed_dim // (self.num_heads * 3)\n",
        "\n",
        "        query, key, value = query_projected.chunk(3, -1)\n",
        "        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
        "        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        if self.training:\n",
        "            dropout = self.dropout\n",
        "            is_causal = self.is_causal\n",
        "        else:\n",
        "            dropout = 0.0\n",
        "            is_causal = False\n",
        "\n",
        "        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n",
        "        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "num_heads = 8\n",
        "heads_per_dim = 64\n",
        "embed_dimension = num_heads * heads_per_dim\n",
        "dtype = torch.float16\n",
        "model = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``NestedTensor`` and Dense tensor support\n",
        "\n",
        "SDPA supports both ``NestedTensor`` and Dense tensor inputs. ``NestedTensors`` handle the case where the input is a batch of variable length sequences\n",
        "without needing to pad each sequence to the maximum length in the batch. For more information about ``NestedTensors`` see\n",
        "[torch.nested](https://pytorch.org/docs/stable/nested.html)_ and [NestedTensors Tutorial](https://pytorch.org/tutorials/prototype/nestedtensor.html)_.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random NT runs in 10915.165 microseconds\n",
            "Random Dense runs in 4556.206 microseconds\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def generate_rand_batch(\n",
        "    batch_size,\n",
        "    max_sequence_len,\n",
        "    embed_dimension,\n",
        "    pad_percentage=None,\n",
        "    dtype=torch.float16,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    if not pad_percentage:\n",
        "        return (\n",
        "            torch.randn(\n",
        "                batch_size,\n",
        "                max_sequence_len,\n",
        "                embed_dimension,\n",
        "                dtype=dtype,\n",
        "                device=device,\n",
        "            ),\n",
        "            None,\n",
        "        )\n",
        "    # Random sequence lengths\n",
        "    seq_len_list = [\n",
        "        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n",
        "        for _ in range(batch_size)\n",
        "    ]\n",
        "    # Make random entry in the batch have max sequence length\n",
        "    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n",
        "    return (\n",
        "        torch.nested.nested_tensor(\n",
        "            [\n",
        "                torch.randn(seq_len, embed_dimension,\n",
        "                            dtype=dtype, device=device)\n",
        "                for seq_len in seq_len_list\n",
        "            ]\n",
        "        ),\n",
        "        seq_len_list,\n",
        "    )\n",
        "\n",
        "random_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device)\n",
        "random_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device)\n",
        "\n",
        "# Currently the fused implementations don't support ``NestedTensor`` for training\n",
        "model.eval()\n",
        "\n",
        "with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n",
        "    try:\n",
        "        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\")\n",
        "        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\")\n",
        "    except RuntimeError:\n",
        "        print(\"FlashAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using SDPA with ``torch.compile``\n",
        "\n",
        "With the release of PyTorch 2.0, a new feature called\n",
        "``torch.compile()`` has been introduced, which can provide\n",
        "significant performance improvements over eager mode.\n",
        "Scaled dot product attention is fully composable with ``torch.compile()``.\n",
        "To demonstrate this, let's compile the ``CausalSelfAttention`` module using\n",
        "``torch.compile()`` and observe the resulting performance improvements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The non compiled module runs in  9041.317 microseconds\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_sequence_len = 256\n",
        "x = torch.rand(batch_size, max_sequence_len,\n",
        "               embed_dimension, device=device, dtype=dtype)\n",
        "print(\n",
        "    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# compiled_model = torch.compile(model)\n",
        "compiled_model = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The compiled module runs in  5193.605 microseconds\n"
          ]
        }
      ],
      "source": [
        "# Let's compile it\n",
        "compiled_model(x)\n",
        "print(\n",
        "    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exact execution time is dependent on machine, however the results for mine:\n",
        "The non compiled module runs in  166.616 microseconds\n",
        "The compiled module runs in  166.726 microseconds\n",
        "That is not what we were expecting. Let's dig a little deeper.\n",
        "PyTorch comes with an amazing built-in profiler that you can use to\n",
        "inspect the performance characteristics of your code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                   Non-Compilied Causal Attention         4.85%       9.004ms       100.00%     185.666ms     185.666ms       5.684ms         2.22%     255.964ms     255.964ms             1  \n",
            "                                     aten::linear         0.69%       1.277ms        40.88%      75.898ms       1.518ms     798.000us         0.31%     158.050ms       3.161ms            50  \n",
            "                                     aten::matmul         1.11%       2.062ms         3.51%       6.515ms     130.300us     939.000us         0.37%     154.397ms       3.088ms            50  \n",
            "                                         aten::mm         1.56%       2.889ms         1.56%       2.889ms      57.780us     151.231ms        59.08%     151.231ms       3.025ms            50  \n",
            "               aten::scaled_dot_product_attention         0.27%     498.000us         4.11%       7.625ms     305.000us     354.000us         0.14%      83.703ms       3.348ms            25  \n",
            "    aten::_scaled_dot_product_efficient_attention         0.84%       1.560ms         3.84%       7.127ms     285.080us       1.124ms         0.44%      83.349ms       3.334ms            25  \n",
            "               aten::_efficient_attention_forward         2.31%       4.292ms         2.47%       4.586ms     183.440us      78.869ms        30.81%      79.927ms       3.197ms            25  \n",
            "                                  aten::transpose        49.78%      92.428ms        49.93%      92.698ms     370.792us       3.462ms         1.35%       6.520ms      26.080us           250  \n",
            "                                      aten::chunk         0.15%     283.000us         1.42%       2.632ms     105.280us     228.000us         0.09%       5.807ms     232.280us            25  \n",
            "                                      aten::split         0.47%     870.000us         1.27%       2.349ms      93.960us     442.000us         0.17%       5.579ms     223.160us            25  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 185.666ms\n",
            "Self CUDA time total: 255.964ms\n",
            "\n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Compiled Causal Attention        13.13%       5.719ms       100.00%      43.552ms      43.552ms       2.525ms         3.96%      63.764ms      63.764ms             1  \n",
            "                                     aten::linear         2.25%     979.000us        69.81%      30.404ms     608.080us     480.000us         0.75%      40.204ms     804.080us            50  \n",
            "                                     aten::matmul         3.22%       1.401ms         7.75%       3.374ms      67.480us     825.000us         1.29%      37.575ms     751.500us            50  \n",
            "                                         aten::mm         3.29%       1.431ms         3.29%       1.431ms      28.620us      35.404ms        55.52%      35.404ms     708.080us            50  \n",
            "               aten::scaled_dot_product_attention         0.60%     260.000us        10.25%       4.462ms     178.480us     227.000us         0.36%      14.824ms     592.960us            25  \n",
            "    aten::_scaled_dot_product_efficient_attention         2.72%       1.183ms         9.65%       4.202ms     168.080us     908.000us         1.42%      14.597ms     583.880us            25  \n",
            "               aten::_efficient_attention_forward         4.28%       1.863ms         4.55%       1.980ms      79.200us      11.415ms        17.90%      12.020ms     480.800us            25  \n",
            "                                  aten::transpose         5.21%       2.270ms         5.54%       2.411ms       9.644us       2.618ms         4.11%       4.918ms      19.672us           250  \n",
            "                                      aten::chunk         0.43%     188.000us         4.59%       1.998ms      79.920us     128.000us         0.20%       3.814ms     152.560us            25  \n",
            "                                      aten::split         1.59%     692.000us         4.16%       1.810ms      72.400us     438.000us         0.69%       3.686ms     147.440us            25  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 43.552ms\n",
            "Self CUDA time total: 63.764ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "activities = [ProfilerActivity.CPU]\n",
        "if device == 'cuda':\n",
        "    activities.append(ProfilerActivity.CUDA)\n",
        "\n",
        "with profile(activities=activities, record_shapes=False) as prof:\n",
        "    with record_function(\" Non-Compilied Causal Attention\"):\n",
        "        for _ in range(25):\n",
        "            model(x)\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "\n",
        "with profile(activities=activities, record_shapes=False) as prof:\n",
        "    with record_function(\"Compiled Causal Attention\"):\n",
        "        for _ in range(25):\n",
        "            compiled_model(x)\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n",
        "# ::\n",
        "#\n",
        "#    prof.export_chrome_trace(\"compiled_causal_attention_trace.json\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous code snippet generates a report of the top 10 PyTorch functions\n",
        "that consumed the most GPU execution time, for both the compiled and non-compiled module.\n",
        "The analysis reveals that the majority of time spent on the GPU is concentrated\n",
        "on the same set of functions for both modules.\n",
        "The reason for this here is that ``torch.compile`` is very good at removing the\n",
        "framework overhead associated with PyTorch. If your model is launching\n",
        "large, efficient CUDA kernels, which in this case ``CausalSelfAttention``\n",
        "is, then the overhead of PyTorch can be hidden.\n",
        "\n",
        "In reality, your module does not normally consist of a singular\n",
        "``CausalSelfAttention`` block. When experimenting with [Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ repository, compiling\n",
        "the module took the time per train step from: ``6090.49ms`` to\n",
        "``3273.17ms``! This was done on commit: ``ae3a8d5`` of NanoGPT training on\n",
        "the Shakespeare dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "In this tutorial, we have demonstrated the basic usage of\n",
        "``torch.nn.functional.scaled_dot_product_attention``. We have shown how\n",
        "the ``sdp_kernel`` context manager can be used to assert a certain\n",
        "implementation is used on GPU. As well, we built a simple\n",
        "``CausalSelfAttention`` module that works with ``NestedTensor`` and is torch\n",
        "compilable. In the process we have shown how to the profiling tools can\n",
        "be used to explore the performance characteristics of a user defined\n",
        "module.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
